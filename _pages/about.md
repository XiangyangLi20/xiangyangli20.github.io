---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>

 I am currently a postdoctoral researcher at the Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences. In 2012, I was graduated from Wuhan Institute of Technology. And then I obtained the M.E. degree from Capital Normal University, Beijing, China, in 2015, advised by Prof. <a href='https://iec.cnu.edu.cn/szdw/bssds/bssds1/105294.htm'>Zhiping Shi</a>. In 2020, I received the Ph.D. degree from the Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, advised by Prof. <a href='http://www.ict.ac.cn/sourcedb_2018_ict_cas/cn/jssrck/200909/t20090917_2496647.html'>Shuqing Jiang</a>.

 My research interests include large-scale image classification, vision and language understanding. I have been serving/served as a reviewer of IEEE TPAMI, IEEE TMM, IEEE TNNLS, IEEE TBD, and ACM TOMM. I also have been serving/served as a PC member of leading conferences in vision, multimedia, and AI, such as CVPR, ICCV, ACM MM, IJCAI, and AAAI.


# üî• News
- *2022.02*: &nbsp;üéâüéâ Our paper <i><strong>Dataset Bias in Few-shot Image Recognition</strong></i> has been accepted by <strong><font color="Blue">IEEE TPAMI</font></strong>!&nbsp;[<a href='https://ieeexplore.ieee.org/document/9720733'>paper</a>]&nbsp;[<a href='http://123.57.42.89/dataset-bias/dataset-bias.html'>project</a>]


# üìù Publications 

**Journal**

- [Dataset Bias in Few-shot Image Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9720733), Shuqiang Jiang, Yaohui Zhu, Chenlong Liu, Xinhang Song, **Xiangyang Li**, Weiqing Min. **TPAMI, 2022** \| [**Project**](http://123.57.42.89/dataset-bias/dataset-bias.html) <strong><span class='show_paper_citations' data='n6WBCgUAAAAJ:hqOjcs7Dif8C'></span></strong>
 &nbsp;<img src='images/databais.png' alt="sym" width="98%">
 &nbsp;In this paper, we first investigate the impact of transferable capabilities learned from base categories. Specifically, we apply word mover‚Äôs distance (WMD) and shortest path length (SPL) to measure the relevance between base categories and novel categories. Distributions of base categories are depicted via the instance density and category diversity. The FSIR model learns better transferable knowledge from relevant training data. In the relevant data, dense instances or diverse categories can further enrich the learned knowledge. Experimental results on different sub-datasets of Imagenet demonstrate category relevance, instance density and category diversity can depict transferable bias from distributions of base categories. Second, we investigate performance differences on different datasets from the aspects of dataset structures and different few-shot learning methods. Specifically, we introduce image complexity, intra-concept visual consistency, and inter-concept visual similarity to quantify characteristics of dataset structures. We use these quantitative characteristics and eight few-shot learning methods to analyze performance differences on multiple datasets. These datasets include MiniCharacter and MiniImagenet which focus on a single object, MiniPlaces which contains many objects in different scales and with varied spatial relationships, as well as MiniFlower and MiniFood which belong to fine-grained datasets. Based on the experimental analysis, some insightful observations are obtained from the perspective of both dataset structures and few-shot learning methods. We hope these observations are useful to guide future few-shot learning research on new datasets or tasks.

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

**Conference**

- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
